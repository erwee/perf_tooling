{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install pandas matplotlib scipy numpy seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas\n",
    "import warnings\n",
    "import requests\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import pprint\n",
    "import seaborn\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../src\")\n",
    "from perf_tools.analysis import make_differential_frame, get_data, get_summary_statistics\n",
    "from perf_tools.analysis import check_are_close, make_latency_plot, plot_latency_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousRWWorkload:\n",
    "    def __init__(self, workdir, patch_id, variant, execution, task_name):\n",
    "        self.workdir = workdir\n",
    "        self.patch_id = patch_id\n",
    "        self.variant = variant\n",
    "        self.execution = execution\n",
    "        self.task_name = task_name\n",
    "        self.insert_data = None\n",
    "        self.find_data = None\n",
    "        self.update_data = None\n",
    "        self.crud_data = None\n",
    "        self.overall_throughput_data = None\n",
    "    def json_path(self, metric):\n",
    "        return os.path.join(self.workdir, self.patch_id, self.variant,\n",
    "            self.task_name, str(self.execution), metric + \".json\")\n",
    "    def get_insert_data(self):\n",
    "        if self.insert_data is None:\n",
    "            self.insert_data = get_data(self.json_path(\"ContinuousRW.insert\"))\n",
    "        return self.insert_data\n",
    "    def get_find_data(self):\n",
    "        if self.find_data is None:\n",
    "            self.find_data = get_data(self.json_path(\"ContinuousRW.find\"))\n",
    "        return self.find_data\n",
    "    def get_update_data(self):\n",
    "        if self.update_data is None:\n",
    "            self.update_data = get_data(self.json_path(\"ContinuousRW.update\"))\n",
    "        return self.update_data\n",
    "    def get_crud_data(self):\n",
    "        if self.crud_data is None:\n",
    "            self.crud_data = get_data(self.json_path(\"ContinuousRW.Crud\"))\n",
    "        return self.crud_data\n",
    "    def get_overall_throughput_data(self):\n",
    "        if self.overall_throughput_data is None:\n",
    "            insert_ops = self.get_insert_data().diff_data[[\"ts\", \"d(ops)\"]]\n",
    "            update_ops = self.get_update_data().diff_data[[\"ts\", \"d(ops)\"]]\n",
    "            find_ops = self.get_find_data().diff_data[[\"ts\", \"d(ops)\"]]\n",
    "            all_ops = pandas.concat([insert_ops, update_ops, find_ops], ignore_index=True)\n",
    "            all_ops.sort_values(\"ts\", inplace=True)\n",
    "            all_ops.reset_index(drop=True, inplace=True)\n",
    "            all_ops[\"duration\"] = (all_ops[\"ts\"] - all_ops[\"ts\"].iloc[0]).astype(int) / 1000000000\n",
    "            all_ops[\"total_ops\"] = all_ops[\"d(ops)\"].cumsum()\n",
    "            all_ops[\"throughput\"] = all_ops[\"total_ops\"] / all_ops[\"duration\"]\n",
    "            self.overall_throughput_data = all_ops\n",
    "        return self.overall_throughput_data\n",
    "\n",
    "    def _plot_line_or_scatter(self, df, x, y, line=False, start=None, end=None, **kwargs):\n",
    "        if line:\n",
    "            return df[start:end].plot(x=x, y=y, figsize=(20,20), **kwargs)\n",
    "        return df[start:end].plot.scatter(x=x, y=y, figsize=(20,20), **kwargs)\n",
    "\n",
    "    def plot_insert_data(self, x, y, line=False, start=None, end=None, **kwargs):\n",
    "        title=f\"{self.variant}-{self.task_name} inserts {y}\"\n",
    "        return self._plot_line_or_scatter(self.get_insert_data().diff_data, x, y, line, start, end, title=title, **kwargs)\n",
    "    def plot_find_data(self, x, y, line=False, start=None, end=None, **kwargs):\n",
    "        title=f\"{self.variant}-{self.task_name} finds {y}\"\n",
    "        return self._plot_line_or_scatter(self.get_find_data().diff_data, x, y, line, start, end, title=title, **kwargs)\n",
    "    def plot_update_data(self, x, y, line=False, start=None, end=None, **kwargs):\n",
    "        title=f\"{self.variant}-{self.task_name} updates {y}\"\n",
    "        return self._plot_line_or_scatter(self.get_update_data().diff_data, x, y, line, start, end, title=title, **kwargs)\n",
    "    def plot_crud_data(self, x, y, line=False, start=None, end=None, **kwargs):\n",
    "        title=f\"{self.variant}-{self.task_name} crud {y}\"\n",
    "        return self._plot_line_or_scatter(self.get_crud_data().diff_data, x, y, line, start, end, title=title, **kwargs)\n",
    "\n",
    "    # Plot the data for insert, find, update\n",
    "    def plot_rw_data(self, x, y, start=None, end=None, noupdate=False):\n",
    "        insert_df = self.get_insert_data().diff_data[start:end]\n",
    "        find_df = self.get_find_data().diff_data[start:end]\n",
    "        plt.figure(figsize=(20,20))\n",
    "        plt.ylabel(y)\n",
    "        plt.xlabel(x)\n",
    "        plt.title(f\"{self.variant}-{self.task_name} inserts, updates, finds {y}\")\n",
    "        if not noupdate:\n",
    "            update_df = self.get_update_data().diff_data[start:end]\n",
    "            plt.plot(update_df[x], update_df[y], alpha=0.8, label=f\"update {y}\")\n",
    "        plt.plot(find_df[x], find_df[y], alpha=0.8, label=f\"find {y}\")\n",
    "        plt.plot(insert_df[x], insert_df[y], alpha=0.8, label=f\"insert {y}\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_throughput_data(self, x, start=None, end=None):\n",
    "        tpdf = self.get_overall_throughput_data()\n",
    "        title=f\"{self.variant}-{self.task_name} overall throughput\"\n",
    "        return tpdf[start:end].plot(x=x, y=\"throughput\", ylabel=\"ops per sec\", title=title, figsize=(20,20))\n",
    "        \n",
    "    def get_insert_summary_statistics(self):\n",
    "        data = self.get_insert_data()\n",
    "        return get_summary_statistics(data.diff_data, data.fixed_data, data.raw_data)\n",
    "    def get_find_summary_statistics(self):\n",
    "        data = self.get_find_data()\n",
    "        return get_summary_statistics(data.diff_data, data.fixed_data, data.raw_data)\n",
    "    def get_update_summary_statistics(self):\n",
    "        data = self.get_update_data()\n",
    "        return get_summary_statistics(data.diff_data, data.fixed_data, data.raw_data)\n",
    "    def get_crud_summary_statistics(self):\n",
    "        data = self.get_crud_data()\n",
    "        return get_summary_statistics(data.diff_data, data.fixed_data, data.raw_data)\n",
    "    def print_all_summary_statistics(self, noupdate=False):\n",
    "        pp = pprint.PrettyPrinter()\n",
    "        print(\"INSERT SUMMARY STATS:\")\n",
    "        pp.pprint(self.get_insert_summary_statistics())\n",
    "        print(\"FIND SUMMARY STATS:\")\n",
    "        pp.pprint(self.get_find_summary_statistics())\n",
    "        if not noupdate:\n",
    "            print(\"UPDATE SUMMARY STATS:\")\n",
    "            pp.pprint(self.get_update_summary_statistics())\n",
    "\n",
    "class ContinuousRWWorkloadWithCompact(ContinuousRWWorkload):\n",
    "    def __init__(self, workdir, patch_id, variant, execution, task_name):\n",
    "        ContinuousRWWorkload.__init__(self, workdir, patch_id, variant, execution, task_name)\n",
    "        self.compact_data = None\n",
    "        self.compacting_find_data = None\n",
    "        self.compacting_update_data = None\n",
    "\n",
    "    def get_compact_data(self):\n",
    "        if self.compact_data is None:\n",
    "            self.compact_data = get_data(self.json_path(\"Compactor.compact\"))\n",
    "        return self.compact_data\n",
    "    def get_compacting_find_data(self):\n",
    "        if self.compacting_find_data is None:\n",
    "            self.compacting_find_data = get_data(self.json_path(\"ContinuousRWCompactInProgress.find\"))\n",
    "        return self.compacting_find_data\n",
    "    def get_compacting_update_data(self):\n",
    "        if self.compacting_update_data is None:\n",
    "            self.compacting_update_data = get_data(self.json_path(\"ContinuousRWCompactInProgress.update\"))\n",
    "        return self.compacting_update_data\n",
    "    def plot_compact_data(self, x, y, line=False, start=None, end=None, **kwargs):\n",
    "        title=f\"{self.variant}-{self.task_name} compacts {y}\"\n",
    "        return self._plot_line_or_scatter(self.get_compact_data().diff_data, x, y, line, start, end, title=title, **kwargs)\n",
    "    def plot_combined_find_data(self, x, y, line=False, start=None, end=None, **kwargs):\n",
    "        ax = self.plot_find_data(x, y, line, start, end, **kwargs)\n",
    "        df = self.get_compacting_find_data().diff_data\n",
    "        return self._plot_line_or_scatter(df, x, y, line, start, end, ax=ax, color=\"orange\", **kwargs)\n",
    "    def plot_combined_update_data(self, x, y, line=False, start=None, end=None, **kwargs):\n",
    "        ax = self.plot_update_data(x, y, line, start, end, **kwargs)\n",
    "        df = self.get_compacting_update_data().diff_data\n",
    "        return self._plot_line_or_scatter(df, x, y, line, start, end, ax=ax, color=\"orange\", **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_dataframe(df):\n",
    "    outfile = \"./temp_dataframe.out\"\n",
    "    with open(outfile, \"wt\") as ostream:\n",
    "        cols = [\"total_ops\", \"ts\", \"actor_id\", \"throughput\", \"duration\", \"pure_latency(ms)\", \"overhead_latency(ms)\", \"total_latency(ms)\"]\n",
    "        df = df[cols].to_string()\n",
    "        ostream.write(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "VARIANTS = {\"replset\": \"linux-3-node-replSet-qebench\", \"sharded\": \"linux-shard-lite-qebench\"}\n",
    "NOCOMPACT_WORKDIR=\"../datasets/genny/continuousrw_nocompact\"\n",
    "COMPACT_WORKDIR=\"../datasets/genny/continuousrw_compact\"\n",
    "\n",
    "nocompact_patchid = \"634405351e2d171a12b2a0a8\"\n",
    "nocompact_taskname = \"genny_qebench_continuousrw_nocompact\"\n",
    "nocompact_replset_wld = ContinuousRWWorkload(NOCOMPACT_WORKDIR, nocompact_patchid, VARIANTS[\"replset\"], 0, nocompact_taskname)\n",
    "nocompact_sharded_wld = ContinuousRWWorkload(NOCOMPACT_WORKDIR, nocompact_patchid, VARIANTS[\"sharded\"], 0, nocompact_taskname)\n",
    "\n",
    "compact_replset_executions = {\n",
    "    \"genny_qebench_continuousrw_compact_1024\" : (\"634451da57e85a772174286f\", [0]),\n",
    "    \"genny_qebench_continuousrw_compact_128\" : (\"634451da57e85a772174286f\", [0]),\n",
    "    \"genny_qebench_continuousrw_compact_256\" : (\"634451da57e85a772174286f\", [0]),\n",
    "    \"genny_qebench_continuousrw_compact_512\" : (\"634451da57e85a772174286f\", [0]),\n",
    "}\n",
    "compact_replset_wlds = {\n",
    "    task: ContinuousRWWorkloadWithCompact(COMPACT_WORKDIR, tup[0], VARIANTS[\"replset\"], tup[1][0], task)\n",
    "    for task, tup in compact_replset_executions.items()\n",
    "}\n",
    "compact_sharded_executions = {\n",
    "    \"genny_qebench_continuousrw_compact_1024\" : (\"6347058fe3c331787727d7c3\", [0]),\n",
    "    \"genny_qebench_continuousrw_compact_128\" : (\"6347058fe3c331787727d7c3\", [1]),\n",
    "    \"genny_qebench_continuousrw_compact_256\" : (\"6347058fe3c331787727d7c3\", [0]),\n",
    "    \"genny_qebench_continuousrw_compact_512\" : (\"6347058fe3c331787727d7c3\", [0]),\n",
    "}\n",
    "compact_sharded_wlds = {\n",
    "    task: ContinuousRWWorkloadWithCompact(COMPACT_WORKDIR, tup[0], VARIANTS[\"sharded\"], tup[1][0], task)\n",
    "    for task, tup in compact_sharded_executions.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row=\"total_ops\"\n",
    "col_latency=\"pure_latency(ms)\"\n",
    "pp = pprint.PrettyPrinter()\n",
    "threads = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nocompact_replset_wld.plot_rw_data(row, col_latency)\n",
    "nocompact_replset_wld.plot_insert_data(row, col_latency)\n",
    "nocompact_replset_wld.plot_find_data(row, col_latency)\n",
    "nocompact_replset_wld.plot_update_data(row, col_latency)\n",
    "nocompact_replset_wld.plot_throughput_data(row)\n",
    "nocompact_replset_wld.print_all_summary_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wld = nocompact_sharded_wld\n",
    "df = wld.get_insert_data().diff_data\n",
    "title=f\"{wld.variant}-{wld.task_name} insert latency stats\"\n",
    "plot_latency_stats(df, row, title=title, regr=\"log\")\n",
    "df = wld.get_find_data().diff_data\n",
    "title=f\"{wld.variant}-{wld.task_name} find latency stats\"\n",
    "plot_latency_stats(df, row, title=title, regr=\"line\")\n",
    "df = wld.get_update_data().diff_data\n",
    "title=f\"{wld.variant}-{wld.task_name} update latency stats\"\n",
    "plot_latency_stats(df, row, title=title, regr=\"line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nocompact_sharded_wld.plot_rw_data(row, col_latency)\n",
    "nocompact_sharded_wld.plot_insert_data(row, col_latency)\n",
    "nocompact_sharded_wld.plot_find_data(row, col_latency)\n",
    "nocompact_sharded_wld.plot_update_data(row, col_latency)\n",
    "nocompact_sharded_wld.plot_throughput_data(row)\n",
    "nocompact_sharded_wld.print_all_summary_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter=[128, 256, 512, 1024]\n",
    "for task, wld in compact_replset_wlds.items():\n",
    "    if int(task.split('_')[-1]) not in filter:\n",
    "        continue\n",
    "    wld.plot_rw_data(row, col_latency)\n",
    "    wld.plot_insert_data(row, col_latency)\n",
    "    wld.plot_combined_find_data(\"ts\", col_latency, True)\n",
    "    wld.plot_combined_update_data(\"ts\", col_latency, True)\n",
    "    wld.plot_compact_data(row, col_latency, False)\n",
    "    wld.print_all_summary_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moving_avg(df, k, x, y):\n",
    "    calc_stats = pandas.DataFrame()\n",
    "    calc_stats[x] = df[x]\n",
    "    calc_stats[\"sma\"] = df[y].rolling(k).mean()\n",
    "    return calc_stats\n",
    "\n",
    "def print_compaction_latency_csv(workload):\n",
    "    compact_df = workload.get_compact_data().diff_data[[\"ts\", \"total_ops\", \"pure_latency(ms)\"]]\n",
    "    compact_df[\"pure_latency(mins)\"] = compact_df[\"pure_latency(ms)\"] / (60*1000)\n",
    "    compact_df.rename(columns = {\"ts\": \"timestamp\"}, inplace=True)\n",
    "    print(compact_df.to_csv(index_label=\"execution\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print compaction latencies as csv\n",
    "for task, wld in compact_replset_wlds.items():\n",
    "    print(task)\n",
    "    print_compaction_latency_csv(wld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how much the moving average latency improves after compaction\n",
    "filter=[128, 256, 512, 1024]\n",
    "for task, wld in compact_replset_wlds.items():\n",
    "    iterations = int(task.split('_')[-1])\n",
    "    if iterations not in filter:\n",
    "        continue\n",
    "\n",
    "    find_df = wld.get_find_data().diff_data\n",
    "    update_df = wld.get_update_data().diff_data\n",
    "    insert_df = wld.get_insert_data().diff_data\n",
    "\n",
    "    # Every CRUD phase performs (iterations * threads) insert+find+update cycles\n",
    "    cycles_per_phase = iterations * threads\n",
    "    num_phases = find_df.shape[0] // cycles_per_phase\n",
    "\n",
    "    # Moving average window size (ie. last k data points to average)\n",
    "    sma_k = 1024\n",
    "\n",
    "    for opname, df in {\"find\": find_df, \"update\": update_df, \"insert\": insert_df}.items():\n",
    "        diff_df = pandas.DataFrame(columns=[\"first sma\", \"max sma\", \"prev max sma\"])\n",
    "        prev_max = None\n",
    "\n",
    "        for i in range(num_phases):\n",
    "            start = i * cycles_per_phase\n",
    "            end = ((i+1) * cycles_per_phase) if i < (num_phases-1) else None\n",
    "            sma_df = get_moving_avg(df[start:end], sma_k, \"ts\", col_latency)\n",
    "            # sma_df.plot(x=\"ts\", figsize=(20,20))\n",
    "            \n",
    "            first_sma = sma_df.iloc[sma_k - 1][\"sma\"]\n",
    "            max_sma = sma_df[\"sma\"].max()\n",
    "            diff_df.loc[i] = [first_sma, max_sma, prev_max]\n",
    "            prev_max = max_sma\n",
    "    \n",
    "        diff_df[\"change\"] = diff_df[\"first sma\"] - diff_df[\"prev max sma\"]\n",
    "        diff_df[\"percent drop\"] = (diff_df[\"change\"] * 100) / diff_df[\"prev max sma\"]\n",
    "\n",
    "        print(f\"Change in {opname} moving mean latency after compaction every {iterations} CRUD iterations:\")\n",
    "        pp.pprint(diff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how much the throughput improves after compaction\n",
    "filter=[512]#[128, 256, 512, 1024]\n",
    "for task, wld in compact_sharded_wlds.items():\n",
    "    iterations = int(task.split('_')[-1])\n",
    "    if iterations not in filter:\n",
    "        continue\n",
    "\n",
    "    crud_phase_dfs = {\n",
    "        \"find\": wld.get_find_data().diff_data,\n",
    "        \"update\": wld.get_update_data().diff_data,\n",
    "        \"insert\": wld.get_insert_data().diff_data\n",
    "    }\n",
    "    compact_phase_crud_dfs = {\n",
    "        \"find\": wld.get_compacting_find_data().diff_data,\n",
    "        \"update\": wld.get_compacting_update_data().diff_data,\n",
    "        \"insert\": None\n",
    "    }\n",
    "    compact_df = wld.get_compact_data().diff_data\n",
    "\n",
    "    # Every CRUD cycle performs (iterations * threads) insert, find, & update operations\n",
    "    ops_per_cycle = iterations * threads\n",
    "    num_crud_cycles = wld.get_crud_data().diff_data.shape[0] // ops_per_cycle\n",
    "    num_compact_cycles = num_crud_cycles - 1\n",
    "\n",
    "    # throughput sample window size (ie. throughput of last N seconds)\n",
    "    duration_secs = 60\n",
    "\n",
    "    for opname, crud_phase_df in crud_phase_dfs.items():\n",
    "        crud_phase_tp_df = pandas.DataFrame(columns=[\"start ts\", \"end ts\", \"tail opcount\", \n",
    "            \"head opcount\", \"tail throughput\", \"head throughput\", \"prev head opcount\",\n",
    "            \"prev head throughput\"])\n",
    "        compact_phase_tp_df = pandas.DataFrame(columns=[\"throughput\", \"compact latency(ms)\"])\n",
    "        prev_head_tp = None\n",
    "        prev_head_opct = None\n",
    "\n",
    "        # The CRUD phase data frame (crud_phase_df) contains metrics for all CRUD cycles.\n",
    "        # To calculate the head and tail throughput for each CRUD cycle, it must\n",
    "        # be segmented into per-cycle data frames (crud_cycle_df).\n",
    "        for i in range(num_crud_cycles):\n",
    "            start = i * ops_per_cycle\n",
    "            end = ((i+1) * ops_per_cycle) if i < (num_crud_cycles-1) else crud_phase_df.shape[0]\n",
    "\n",
    "            crud_cycle_df = crud_phase_df[start:end]\n",
    "\n",
    "            start_ts = crud_cycle_df[\"ts\"].iloc[0]\n",
    "            end_ts = crud_cycle_df[\"ts\"].iloc[-1]\n",
    "\n",
    "            # calculate the tail throughput, or the throughput at the first duration_secs of the cycle\n",
    "            cutoff_ts = start_ts + pandas.DateOffset(seconds=duration_secs)\n",
    "            tail_opct = crud_cycle_df[crud_cycle_df[\"ts\"] <= cutoff_ts].shape[0]\n",
    "            tail_tp = tail_opct / duration_secs\n",
    "\n",
    "            # calculate the head throughput, or the throughput at the last duration_secs of the cycle\n",
    "            cutoff_ts = end_ts - pandas.DateOffset(seconds=duration_secs)\n",
    "            head_opct = crud_cycle_df[crud_cycle_df[\"ts\"] >= cutoff_ts].shape[0]\n",
    "            head_tp = head_opct / duration_secs\n",
    "\n",
    "            # add calc'd values as a row in the throughput dataframe\n",
    "            crud_phase_tp_df.loc[i] = [start_ts, end_ts, tail_opct, head_opct, tail_tp, head_tp, \n",
    "                prev_head_opct, prev_head_tp]\n",
    "            prev_head_opct = head_opct\n",
    "            prev_head_tp = head_tp\n",
    "\n",
    "        # The Compact phase CRUD data frame (compact_phase_crud_df) contains metrics for\n",
    "        # all CRUD cycles happening while compaction is in progress.\n",
    "        # To calculate the throughput, it must first be segmented into per-cycle data frames.\n",
    "        # The boundary of each segment is determined by the last and first timestamps of the\n",
    "        # CRUD phase cycles that preceded and followed the current Compact phase cycle.\n",
    "        for i in range(num_compact_cycles):\n",
    "            compact_phase_crud_df = compact_phase_crud_dfs[opname]\n",
    "            tp = None\n",
    "            if compact_phase_crud_df is not None:\n",
    "                # The end timestamp of the CRUD phase cycle that immediately preceded\n",
    "                # this Compact phase cycle is the lower bound of this segment.\n",
    "                start_ts = crud_phase_tp_df.iloc[i][\"end ts\"]\n",
    "\n",
    "                # The start timestamp of the CRUD phase cycle that immediately followed\n",
    "                # this Compact phase sycle is the upper bound of this segment.\n",
    "                end_ts = crud_phase_tp_df.iloc[i+1][\"start ts\"]\n",
    "\n",
    "                crud_cycle_df = compact_phase_crud_df.loc[(compact_phase_crud_df[\"ts\"] >= start_ts) & (compact_phase_crud_df[\"ts\"] <= end_ts)]\n",
    "                tp = crud_cycle_df.shape[0] / (end_ts - start_ts).total_seconds()\n",
    "\n",
    "            compact_phase_tp_df.loc[i] = [tp, compact_df.iloc[i][\"pure_latency(ms)\"]]\n",
    "\n",
    "        crud_phase_tp_df[\"change\"] = crud_phase_tp_df[\"tail throughput\"] - crud_phase_tp_df[\"prev head throughput\"]\n",
    "        crud_phase_tp_df[\"percent change\"] = (crud_phase_tp_df[\"change\"] * 100) / crud_phase_tp_df[\"prev head throughput\"]\n",
    "\n",
    "        agg_tp_df = pandas.DataFrame()\n",
    "        crud_phase_tp_df = crud_phase_tp_df[1:].reset_index()\n",
    "        agg_tp_df[\"compact latency(ms)\"] = compact_phase_tp_df[\"compact latency(ms)\"]\n",
    "        agg_tp_df[\"latency change\"] = agg_tp_df[\"compact latency(ms)\"].diff()\n",
    "        agg_tp_df[\"opcount before\"] = crud_phase_tp_df[\"prev head opcount\"]\n",
    "        agg_tp_df[\"opcount after\"] = crud_phase_tp_df[\"tail opcount\"]\n",
    "        agg_tp_df[\"throughput window(secs)\"] = duration_secs\n",
    "        agg_tp_df[\"throughput before\"] = crud_phase_tp_df[\"prev head throughput\"]\n",
    "        agg_tp_df = agg_tp_df.reindex()\n",
    "        agg_tp_df[\"throughput during\"] = compact_phase_tp_df[\"throughput\"]\n",
    "        agg_tp_df[\"throughput after\"] = crud_phase_tp_df[\"tail throughput\"]\n",
    "        agg_tp_df[\"throughput change\"] = crud_phase_tp_df[\"change\"]\n",
    "        agg_tp_df[\"throughput percent change\"] = crud_phase_tp_df[\"percent change\"]\n",
    "\n",
    "        title = f\"Change in {opname} throughput vs compaction latency: N={iterations}\"\n",
    "        print(f\"{title}:\")\n",
    "\n",
    "        ax = agg_tp_df[[\"throughput change\", \"throughput percent change\", \"compact latency(ms)\"]].plot(\n",
    "            xlabel=\"nth compaction\", xticks=agg_tp_df.index, sharex=False, figsize=(10,10), title=title, subplots=True)\n",
    "        ax[0].set_ylabel(\"ops per second\")\n",
    "        ax[0].set_xticks(agg_tp_df.index)\n",
    "        ax[1].set_ylabel(\"percent\")\n",
    "        ax[2].set_ylabel(\"milliseconds\")\n",
    "\n",
    "        agg_tp_df[[\"throughput before\", \"throughput during\", \"throughput after\"]].plot(\n",
    "            xticks=agg_tp_df.index, sharex=False, figsize=(10,10), title=title)\n",
    "\n",
    "        print(agg_tp_df.to_csv(index_label=\"compact execution\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total throughput after 100k CRUD cycles with different\n",
    "# compaction intervals.\n",
    "# Compare with the base workload that performed no compaction.\n",
    "\n",
    "base_df = nocompact_replset_wld.get_crud_data().diff_data\n",
    "base_df = base_df[base_df[\"total_ops\"] == 100000][[\"duration\", \"total_ops\", \"throughput\"]]\n",
    "print(\"=== Throughput of 100k CRUD cycles without compaction:\")\n",
    "pp.pprint(base_df)\n",
    "\n",
    "filter=[128, 256, 512, 1024]\n",
    "for task, wld in compact_replset_wlds.items():\n",
    "    iterations = int(task.split('_')[-1])\n",
    "    if iterations not in filter:\n",
    "        continue\n",
    "    cycles_per_phase = iterations * threads\n",
    "\n",
    "    measure_df = wld.get_crud_data().diff_data\n",
    "    measure_df = measure_df[measure_df[\"total_ops\"] == 100000][[\"duration\", \"total_ops\", \"throughput\"]]\n",
    "    print(f\"=== Throughput of 100k CRUD cycles with compaction every {cycles_per_phase} cycles:\")\n",
    "    pp.pprint(measure_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter=[128, 256]\n",
    "for task, wld in compact_sharded_wlds.items():\n",
    "    if int(task.split('_')[-1]) not in filter:\n",
    "        continue\n",
    "    wld.plot_rw_data(row, col_latency)\n",
    "    wld.plot_insert_data(row, col_latency)\n",
    "    wld.plot_combined_find_data(\"ts\", col_latency, True)\n",
    "    wld.plot_combined_update_data(\"ts\", col_latency, True)\n",
    "    wld.plot_compact_data(row, col_latency, False)\n",
    "    wld.print_all_summary_statistics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 ('venv': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a9de337d49c809f126ef412fa5b70dd3b8eac743dbb23eeffda322bc3293c07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
